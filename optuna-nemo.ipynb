{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "import torch.multiprocessing as mp\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronTrainerBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "from nemo.utils import logging\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "from nemo.core.config import hydra_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hydra_runner(config_path=\"/global/scratch/users/ksevegnani/nemo_test\", config_name=\"llama2_7b.yaml\")\n",
    "# def get_config(cfg):\n",
    "#     print(f'\\n{OmegaConf.to_yaml(cfg)}')\n",
    "#     return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    with initialize(version_base=None, config_path=\"nemo_test\"):\n",
    "        cfg = compose(config_name=\"llama2_7b_optuna.yaml\")\n",
    "    print(f'\\n{OmegaConf.to_yaml(cfg)}')\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(cfg, trainer):\n",
    "    logging.info(\"\\n\\n************** Experiment configuration ***********\")\n",
    "    logging.info(f'\\n{OmegaConf.to_yaml(cfg)}')\n",
    "    \n",
    "    # Continual training\n",
    "    if cfg.model.get(\"restore_from_path\") is not None:\n",
    "        # Option 1: Restore only the model weights from a .nemo file\n",
    "        logging.info(f\"Continual training: loading weights from {cfg.model.restore_from_path}\")\n",
    "        model = MegatronGPTModel.restore_from(\n",
    "            restore_path=cfg.model.restore_from_path,\n",
    "            override_config_path=cfg.model,\n",
    "            trainer=trainer,\n",
    "            save_restore_connector=NLPSaveRestoreConnector(),\n",
    "        )\n",
    "    elif cfg.model.get(\"restore_from_ckpt\") is not None:\n",
    "        # Option 2: Restore both model weights and optimizer states from a PTL checkpoint\n",
    "        logging.info(f\"Continual training: loading weights and optimizer states from {cfg.model.restore_from_ckpt}\")\n",
    "        trainer.ckpt_path = Path(cfg.model.restore_from_ckpt)\n",
    "        model = MegatronGPTModel(cfg.model, trainer)\n",
    "    else:\n",
    "        # Start new pretraining or resume from a checkpoint if it exists\n",
    "        model = MegatronGPTModel(cfg.model, trainer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    cfg = get_config()\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.2)\n",
    "\n",
    "    \n",
    "\n",
    "    trainer = MegatronTrainerBuilder(cfg).create_trainer()\n",
    "    exp_manager(trainer, cfg.exp_manager)\n",
    "\n",
    "    # Load the pre-trained Llama 2 model\n",
    "    model = initialize_model(cfg, trainer)\n",
    "\n",
    "    # Configure the model with suggested hyperparameters\n",
    "    model.cfg.optim.lr = learning_rate\n",
    "    model.cfg.optim.weight_decay = weight_decay\n",
    "    model.cfg.optim.sched.warmup_ratio = warmup_ratio\n",
    "\n",
    "    # Set up the trainer\n",
    "    #     trainer = pl.Trainer(\n",
    "    #         max_epochs=3,\n",
    "    #         gpus=1,\n",
    "    #         precision=16,\n",
    "    #         amp_level='O2',\n",
    "    #         accelerator=\"gpu\",\n",
    "    #         strategy=\"ddp\",\n",
    "    #         log_every_n_steps=10,\n",
    "    #         val_check_interval=0.5,\n",
    "    #     )\n",
    "\n",
    "\n",
    "        # Set up the experiment manager\n",
    "    #         exp_manager(\n",
    "    #             trainer,\n",
    "    #             exp_dir=\"optuna_experiments\",\n",
    "    #             create_tensorboard_logger=True,\n",
    "    #             create_wandb_logger=False,\n",
    "    #         )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.fit(model)\n",
    "\n",
    "    # Return the validation loss as the objective value\n",
    "    return trainer.callback_metrics['val_loss'].item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 07:05:29,367] A new study created in memory with name: no-name-61cd6ea9-a1f2-4314-bff1-726557949040\n"
     ]
    }
   ],
   "source": [
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run:\n",
      "  name: llama2_7b\n",
      "  results_dir: /global/scratch/users/ksevegnani/nemo_test/out\n",
      "  time_limit: 0-01:30:00\n",
      "  dependency: singleton\n",
      "trainer:\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  accelerator: gpu\n",
      "  precision: bf16\n",
      "  logger: false\n",
      "  enable_checkpointing: false\n",
      "  use_distributed_sampler: false\n",
      "  max_epochs: null\n",
      "  max_steps: 100\n",
      "  max_time: 05:23:30:00\n",
      "  log_every_n_steps: 10\n",
      "  val_check_interval: 100\n",
      "  limit_val_batches: 32\n",
      "  limit_test_batches: 50\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 1.0\n",
      "exp_manager:\n",
      "  explicit_log_dir: ${run.results_dir}/results\n",
      "  exp_dir: null\n",
      "  name: megatron_llama\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    project: nemo_llama_pretrain\n",
      "    name: ${run.name}\n",
      "  resume_if_exists: false\n",
      "  resume_ignore_no_checkpoint: true\n",
      "  create_checkpoint_callback: true\n",
      "  checkpoint_callback_params:\n",
      "    monitor: val_loss\n",
      "    save_top_k: 10\n",
      "    mode: min\n",
      "    always_save_nemo: false\n",
      "    save_nemo_on_train_end: false\n",
      "    filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "  log_step_timing: true\n",
      "  step_timing_kwargs:\n",
      "    sync_cuda: true\n",
      "    buffer_size: 5\n",
      "model:\n",
      "  mcore_gpt: true\n",
      "  micro_batch_size: 1\n",
      "  global_batch_size: 1\n",
      "  rampup_batch_size: null\n",
      "  tensor_model_parallel_size: 1\n",
      "  pipeline_model_parallel_size: 1\n",
      "  virtual_pipeline_model_parallel_size: null\n",
      "  encoder_seq_length: 2048\n",
      "  max_position_embeddings: 2048\n",
      "  num_layers: 4\n",
      "  hidden_size: 2048\n",
      "  ffn_hidden_size: 11008\n",
      "  num_attention_heads: 32\n",
      "  init_method_std: 0.01\n",
      "  use_scaled_init_method: true\n",
      "  hidden_dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  ffn_dropout: 0.0\n",
      "  kv_channels: null\n",
      "  apply_query_key_layer_scaling: true\n",
      "  normalization: rmsnorm\n",
      "  layernorm_epsilon: 1.0e-05\n",
      "  do_layer_norm_weight_decay: false\n",
      "  make_vocab_size_divisible_by: 128\n",
      "  pre_process: true\n",
      "  post_process: true\n",
      "  persist_layer_norm: true\n",
      "  bias: false\n",
      "  activation: fast-swiglu\n",
      "  headscale: false\n",
      "  transformer_block_type: pre_ln\n",
      "  openai_gelu: false\n",
      "  normalize_attention_scores: true\n",
      "  position_embedding_type: rope\n",
      "  rotary_percentage: 1.0\n",
      "  apply_rope_fusion: true\n",
      "  attention_type: multihead\n",
      "  share_embeddings_and_output_weights: false\n",
      "  tokenizer:\n",
      "    library: sentencepiece\n",
      "    type: null\n",
      "    model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "    delimiter: null\n",
      "    vocab_file: null\n",
      "    merge_file: null\n",
      "    sentencepiece_legacy: false\n",
      "  native_amp_init_scale: 4294967296\n",
      "  native_amp_growth_interval: 1000\n",
      "  hysteresis: 2\n",
      "  fp32_residual_connection: false\n",
      "  fp16_lm_cross_entropy: false\n",
      "  megatron_amp_O2: true\n",
      "  grad_allreduce_chunk_size_mb: 125\n",
      "  grad_div_ar_fusion: true\n",
      "  gradient_accumulation_fusion: true\n",
      "  bias_activation_fusion: true\n",
      "  bias_dropout_add_fusion: true\n",
      "  masked_softmax_fusion: true\n",
      "  seed: 1234\n",
      "  resume_from_checkpoint: null\n",
      "  use_cpu_initialization: false\n",
      "  onnx_safe: false\n",
      "  apex_transformer_log_level: 30\n",
      "  gradient_as_bucket_view: true\n",
      "  sync_batch_comm: false\n",
      "  activations_checkpoint_granularity: null\n",
      "  activations_checkpoint_method: block\n",
      "  activations_checkpoint_num_layers: 0\n",
      "  num_micro_batches_with_partial_activation_checkpoints: null\n",
      "  activations_checkpoint_layers_per_pipeline: null\n",
      "  sequence_parallel: false\n",
      "  transformer_engine: true\n",
      "  fp8: true\n",
      "  fp8_e4m3: true\n",
      "  fp8_hybrid: false\n",
      "  fp8_margin: 0\n",
      "  fp8_interval: 1\n",
      "  fp8_amax_history_len: 1024\n",
      "  fp8_amax_compute_algo: max\n",
      "  use_emha: false\n",
      "  ub_tp_comm_overlap: false\n",
      "  tp_comm_atomic_ag: false\n",
      "  tp_comm_atomic_rs: false\n",
      "  use_flash_attention: true\n",
      "  optim:\n",
      "    name: distributed_fused_adam\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.1\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.95\n",
      "    bucket_cap_mb: 125\n",
      "    overlap_grad_sync: true\n",
      "    overlap_param_sync: true\n",
      "    contiguous_grad_buffer: true\n",
      "    contiguous_param_buffer: true\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_ratio: 0.1\n",
      "      constant_steps: 0\n",
      "      min_lr: 1.0e-05\n",
      "  data:\n",
      "    data_impl: mmap\n",
      "    splits_string: 99,1,1\n",
      "    seq_length: 4096\n",
      "    skip_warmup: true\n",
      "    num_workers: 2\n",
      "    dataloader_type: single\n",
      "    reset_position_ids: false\n",
      "    reset_attention_mask: false\n",
      "    eod_mask_loss: false\n",
      "    index_mapping_dir: null\n",
      "    data_prefix:\n",
      "    - 1.0\n",
      "    - /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document\n",
      "\n",
      "[NeMo I 2024-10-15 07:05:29 megatron_trainer_builder:58] Detected interactive environment, using NLPDDPStrategyNotebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:29 dist_ckpt_io:320] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:29 exp_manager:341] ExpManager schema\n",
      "[NeMo I 2024-10-15 07:05:29 exp_manager:342] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:30 exp_manager:712] Exp_manager is logging to /global/scratch/users/ksevegnani/nemo_test/out/results, but it already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:30 exp_manager:400] Experiments will be logged at /global/scratch/users/ksevegnani/nemo_test/out/results\n",
      "[NeMo I 2024-10-15 07:05:30 exp_manager:860] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:30 exp_manager:970] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:30 3748006741:2] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-15 07:05:30 3748006741:3] \n",
      "    run:\n",
      "      name: llama2_7b\n",
      "      results_dir: /global/scratch/users/ksevegnani/nemo_test/out\n",
      "      time_limit: 0-01:30:00\n",
      "      dependency: singleton\n",
      "    trainer:\n",
      "      num_nodes: 1\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 100\n",
      "      max_time: 05:23:30:00\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 32\n",
      "      limit_test_batches: 50\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: ${run.results_dir}/results\n",
      "      exp_dir: null\n",
      "      name: megatron_llama\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: nemo_llama_pretrain\n",
      "        name: ${run.name}\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "      log_step_timing: true\n",
      "      step_timing_kwargs:\n",
      "        sync_cuda: true\n",
      "        buffer_size: 5\n",
      "    model:\n",
      "      mcore_gpt: true\n",
      "      micro_batch_size: 1\n",
      "      global_batch_size: 1\n",
      "      rampup_batch_size: null\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 2048\n",
      "      max_position_embeddings: 2048\n",
      "      num_layers: 4\n",
      "      hidden_size: 2048\n",
      "      ffn_hidden_size: 11008\n",
      "      num_attention_heads: 32\n",
      "      init_method_std: 0.01\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      normalization: rmsnorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      bias: false\n",
      "      activation: fast-swiglu\n",
      "      headscale: false\n",
      "      transformer_block_type: pre_ln\n",
      "      openai_gelu: false\n",
      "      normalize_attention_scores: true\n",
      "      position_embedding_type: rope\n",
      "      rotary_percentage: 1.0\n",
      "      apply_rope_fusion: true\n",
      "      attention_type: multihead\n",
      "      share_embeddings_and_output_weights: false\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "        delimiter: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        sentencepiece_legacy: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: true\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: true\n",
      "      gradient_accumulation_fusion: true\n",
      "      bias_activation_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: null\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: block\n",
      "      activations_checkpoint_num_layers: 0\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: false\n",
      "      transformer_engine: true\n",
      "      fp8: true\n",
      "      fp8_e4m3: true\n",
      "      fp8_hybrid: false\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1024\n",
      "      fp8_amax_compute_algo: max\n",
      "      use_emha: false\n",
      "      ub_tp_comm_overlap: false\n",
      "      tp_comm_atomic_ag: false\n",
      "      tp_comm_atomic_rs: false\n",
      "      use_flash_attention: true\n",
      "      optim:\n",
      "        name: distributed_fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.1\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        bucket_cap_mb: 125\n",
      "        overlap_grad_sync: true\n",
      "        overlap_param_sync: true\n",
      "        contiguous_grad_buffer: true\n",
      "        contiguous_param_buffer: true\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_ratio: 0.1\n",
      "          constant_steps: 0\n",
      "          min_lr: 1.0e-05\n",
      "      data:\n",
      "        data_impl: mmap\n",
      "        splits_string: 99,1,1\n",
      "        seq_length: 4096\n",
      "        skip_warmup: true\n",
      "        num_workers: 2\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        index_mapping_dir: null\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:30 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:05:30 megatron_init:352] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1015 07:05:30.308953700 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:30 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:31 tokenizer_utils:188] Getting SentencePiece with model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "[NeMo I 2024-10-15 07:05:31 megatron_base_model:584] Padded vocab_size: 32128, original vocab_size: 32003, dummy tokens: 125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:498] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:31 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:05:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `MegatronGPTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:05:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:163: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2024-10-15 07:05:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints exists and is not empty.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:56 megatron_gpt_model:1592] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 4.69e+08. Number of precise model parameters on device: 469256192.\n",
      "[NeMo I 2024-10-15 07:05:56 megatron_gpt_model:1446] Building GPT datasets.\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] Let split_matrix = [(0, 0.9801980198019802), (0.9801980198019802, 0.9900990099009901), (0.9900990099009901, 1.0)]\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] Building dataset splits with cls=GPTDataset, sizes=[100, 64, 50], and config=GPTDatasetConfig(random_seed=1234, sequence_length=4096, blend=(['/global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document'], [1.0]), blend_per_split=None, split='99,1,1', split_matrix=[(0, 0.9801980198019802), (0.9801980198019802, 0.9900990099009901), (0.9900990099009901, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.sentencepiece_tokenizer.SentencePieceTokenizer object at 0x51867c765540>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True)\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] Load the _IndexReader from /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document.idx\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tExtract the sequence lengths\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tExtract the sequence pointers\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tExtract the document indices\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] > total number of sequences: 1000\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] > total number of documents: 1000\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] Load the GPTDataset train indices\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tLoad the document index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-document_index.npy\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tLoad the sample index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tLoad the shuffle index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] > total number of samples: 803\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] Load the GPTDataset valid indices\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tLoad the document index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-document_index.npy\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tLoad the sample index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] \tLoad the shuffle index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:05:56 utils:47] > total number of samples: 72\n",
      "[NeMo I 2024-10-15 07:05:57 utils:47] Load the GPTDataset test indices\n",
      "[NeMo I 2024-10-15 07:05:57 utils:47] \tLoad the document index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-document_index.npy\n",
      "[NeMo I 2024-10-15 07:05:57 utils:47] \tLoad the sample index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:05:57 utils:47] \tLoad the shuffle index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:05:57 utils:47] > total number of samples: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:57 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:57 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:05:57 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:57 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-10-15 07:05:57 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:57 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:05:57 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:57 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-10-15 07:05:57 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:57 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:05:57 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:57 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1530] Length of train dataset: 101\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1532] Length of val dataset: 65\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1534] Length of test dataset: 51\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1535] Finished building GPT datasets.\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1636] Setting up train dataloader with len(len(self._train_ds)): 101 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:05:57 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 101 and consumed_samples: 0\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1644] Setting up validation dataloader with len(len(self._validation_ds)): 65 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:05:57 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 65 and consumed_samples: 0\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1665] Setting up test dataloader with len(len(self._test_ds)): 51 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:05:57 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:05:57 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 51 and consumed_samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:05:57 modelPT:770] Optimizer config = MegatronDistributedFusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 0.0003654503479685462\n",
      "        weight_decay: 0.0009979655831599337\n",
      "    )\n",
      "[NeMo I 2024-10-15 07:05:57 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x51869e1ab4f0>\" \n",
      "    will be used during training (effective maximum steps = 100) - \n",
      "    Parameters : \n",
      "    (warmup_ratio: 0.1911978919333179\n",
      "    constant_steps: 0\n",
      "    min_lr: 1.0e-05\n",
      "    max_steps: 100\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | Float16Module | 469 M \n",
      "----------------------------------------\n",
      "469 M     Trainable params\n",
      "0         Non-trainable params\n",
      "469 M     Total params\n",
      "1,877.025 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.004471302032470703,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:05:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:06:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:06:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.003329753875732422,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3819431d383c4d57b2b32b3cd0ae9d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0034062862396240234,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'val_loss' reached 5.90896 (best 5.90896), saving model to '/global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints/megatron_llama--val_loss=5.91-step=100-consumed_samples=100.0.ckpt' as top 10\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "[I 2024-10-15 07:06:41,517] Trial 0 finished with value: 5.908960819244385 and parameters: {'learning_rate': 0.0003654503479685462, 'weight_decay': 0.0009979655831599337, 'warmup_ratio': 0.1911978919333179}. Best is trial 0 with value: 5.908960819244385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run:\n",
      "  name: llama2_7b\n",
      "  results_dir: /global/scratch/users/ksevegnani/nemo_test/out\n",
      "  time_limit: 0-01:30:00\n",
      "  dependency: singleton\n",
      "trainer:\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  accelerator: gpu\n",
      "  precision: bf16\n",
      "  logger: false\n",
      "  enable_checkpointing: false\n",
      "  use_distributed_sampler: false\n",
      "  max_epochs: null\n",
      "  max_steps: 100\n",
      "  max_time: 05:23:30:00\n",
      "  log_every_n_steps: 10\n",
      "  val_check_interval: 100\n",
      "  limit_val_batches: 32\n",
      "  limit_test_batches: 50\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 1.0\n",
      "exp_manager:\n",
      "  explicit_log_dir: ${run.results_dir}/results\n",
      "  exp_dir: null\n",
      "  name: megatron_llama\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    project: nemo_llama_pretrain\n",
      "    name: ${run.name}\n",
      "  resume_if_exists: false\n",
      "  resume_ignore_no_checkpoint: true\n",
      "  create_checkpoint_callback: true\n",
      "  checkpoint_callback_params:\n",
      "    monitor: val_loss\n",
      "    save_top_k: 10\n",
      "    mode: min\n",
      "    always_save_nemo: false\n",
      "    save_nemo_on_train_end: false\n",
      "    filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "  log_step_timing: true\n",
      "  step_timing_kwargs:\n",
      "    sync_cuda: true\n",
      "    buffer_size: 5\n",
      "model:\n",
      "  mcore_gpt: true\n",
      "  micro_batch_size: 1\n",
      "  global_batch_size: 1\n",
      "  rampup_batch_size: null\n",
      "  tensor_model_parallel_size: 1\n",
      "  pipeline_model_parallel_size: 1\n",
      "  virtual_pipeline_model_parallel_size: null\n",
      "  encoder_seq_length: 2048\n",
      "  max_position_embeddings: 2048\n",
      "  num_layers: 4\n",
      "  hidden_size: 2048\n",
      "  ffn_hidden_size: 11008\n",
      "  num_attention_heads: 32\n",
      "  init_method_std: 0.01\n",
      "  use_scaled_init_method: true\n",
      "  hidden_dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  ffn_dropout: 0.0\n",
      "  kv_channels: null\n",
      "  apply_query_key_layer_scaling: true\n",
      "  normalization: rmsnorm\n",
      "  layernorm_epsilon: 1.0e-05\n",
      "  do_layer_norm_weight_decay: false\n",
      "  make_vocab_size_divisible_by: 128\n",
      "  pre_process: true\n",
      "  post_process: true\n",
      "  persist_layer_norm: true\n",
      "  bias: false\n",
      "  activation: fast-swiglu\n",
      "  headscale: false\n",
      "  transformer_block_type: pre_ln\n",
      "  openai_gelu: false\n",
      "  normalize_attention_scores: true\n",
      "  position_embedding_type: rope\n",
      "  rotary_percentage: 1.0\n",
      "  apply_rope_fusion: true\n",
      "  attention_type: multihead\n",
      "  share_embeddings_and_output_weights: false\n",
      "  tokenizer:\n",
      "    library: sentencepiece\n",
      "    type: null\n",
      "    model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "    delimiter: null\n",
      "    vocab_file: null\n",
      "    merge_file: null\n",
      "    sentencepiece_legacy: false\n",
      "  native_amp_init_scale: 4294967296\n",
      "  native_amp_growth_interval: 1000\n",
      "  hysteresis: 2\n",
      "  fp32_residual_connection: false\n",
      "  fp16_lm_cross_entropy: false\n",
      "  megatron_amp_O2: true\n",
      "  grad_allreduce_chunk_size_mb: 125\n",
      "  grad_div_ar_fusion: true\n",
      "  gradient_accumulation_fusion: true\n",
      "  bias_activation_fusion: true\n",
      "  bias_dropout_add_fusion: true\n",
      "  masked_softmax_fusion: true\n",
      "  seed: 1234\n",
      "  resume_from_checkpoint: null\n",
      "  use_cpu_initialization: false\n",
      "  onnx_safe: false\n",
      "  apex_transformer_log_level: 30\n",
      "  gradient_as_bucket_view: true\n",
      "  sync_batch_comm: false\n",
      "  activations_checkpoint_granularity: null\n",
      "  activations_checkpoint_method: block\n",
      "  activations_checkpoint_num_layers: 0\n",
      "  num_micro_batches_with_partial_activation_checkpoints: null\n",
      "  activations_checkpoint_layers_per_pipeline: null\n",
      "  sequence_parallel: false\n",
      "  transformer_engine: true\n",
      "  fp8: true\n",
      "  fp8_e4m3: true\n",
      "  fp8_hybrid: false\n",
      "  fp8_margin: 0\n",
      "  fp8_interval: 1\n",
      "  fp8_amax_history_len: 1024\n",
      "  fp8_amax_compute_algo: max\n",
      "  use_emha: false\n",
      "  ub_tp_comm_overlap: false\n",
      "  tp_comm_atomic_ag: false\n",
      "  tp_comm_atomic_rs: false\n",
      "  use_flash_attention: true\n",
      "  optim:\n",
      "    name: distributed_fused_adam\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.1\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.95\n",
      "    bucket_cap_mb: 125\n",
      "    overlap_grad_sync: true\n",
      "    overlap_param_sync: true\n",
      "    contiguous_grad_buffer: true\n",
      "    contiguous_param_buffer: true\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_ratio: 0.1\n",
      "      constant_steps: 0\n",
      "      min_lr: 1.0e-05\n",
      "  data:\n",
      "    data_impl: mmap\n",
      "    splits_string: 99,1,1\n",
      "    seq_length: 4096\n",
      "    skip_warmup: true\n",
      "    num_workers: 2\n",
      "    dataloader_type: single\n",
      "    reset_position_ids: false\n",
      "    reset_attention_mask: false\n",
      "    eod_mask_loss: false\n",
      "    index_mapping_dir: null\n",
      "    data_prefix:\n",
      "    - 1.0\n",
      "    - /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document\n",
      "\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_trainer_builder:58] Detected interactive environment, using NLPDDPStrategyNotebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:41 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:41 dist_ckpt_io:320] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:41 exp_manager:341] ExpManager schema\n",
      "[NeMo I 2024-10-15 07:06:41 exp_manager:342] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:41 exp_manager:712] Exp_manager is logging to /global/scratch/users/ksevegnani/nemo_test/out/results, but it already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:41 exp_manager:400] Experiments will be logged at /global/scratch/users/ksevegnani/nemo_test/out/results\n",
      "[NeMo I 2024-10-15 07:06:41 exp_manager:860] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:41 exp_manager:970] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:41 3748006741:2] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-15 07:06:41 3748006741:3] \n",
      "    run:\n",
      "      name: llama2_7b\n",
      "      results_dir: /global/scratch/users/ksevegnani/nemo_test/out\n",
      "      time_limit: 0-01:30:00\n",
      "      dependency: singleton\n",
      "    trainer:\n",
      "      num_nodes: 1\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 100\n",
      "      max_time: 05:23:30:00\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 32\n",
      "      limit_test_batches: 50\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: ${run.results_dir}/results\n",
      "      exp_dir: null\n",
      "      name: megatron_llama\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: nemo_llama_pretrain\n",
      "        name: ${run.name}\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "      log_step_timing: true\n",
      "      step_timing_kwargs:\n",
      "        sync_cuda: true\n",
      "        buffer_size: 5\n",
      "    model:\n",
      "      mcore_gpt: true\n",
      "      micro_batch_size: 1\n",
      "      global_batch_size: 1\n",
      "      rampup_batch_size: null\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 2048\n",
      "      max_position_embeddings: 2048\n",
      "      num_layers: 4\n",
      "      hidden_size: 2048\n",
      "      ffn_hidden_size: 11008\n",
      "      num_attention_heads: 32\n",
      "      init_method_std: 0.01\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      normalization: rmsnorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      bias: false\n",
      "      activation: fast-swiglu\n",
      "      headscale: false\n",
      "      transformer_block_type: pre_ln\n",
      "      openai_gelu: false\n",
      "      normalize_attention_scores: true\n",
      "      position_embedding_type: rope\n",
      "      rotary_percentage: 1.0\n",
      "      apply_rope_fusion: true\n",
      "      attention_type: multihead\n",
      "      share_embeddings_and_output_weights: false\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "        delimiter: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        sentencepiece_legacy: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: true\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: true\n",
      "      gradient_accumulation_fusion: true\n",
      "      bias_activation_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: null\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: block\n",
      "      activations_checkpoint_num_layers: 0\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: false\n",
      "      transformer_engine: true\n",
      "      fp8: true\n",
      "      fp8_e4m3: true\n",
      "      fp8_hybrid: false\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1024\n",
      "      fp8_amax_compute_algo: max\n",
      "      use_emha: false\n",
      "      ub_tp_comm_overlap: false\n",
      "      tp_comm_atomic_ag: false\n",
      "      tp_comm_atomic_rs: false\n",
      "      use_flash_attention: true\n",
      "      optim:\n",
      "        name: distributed_fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.1\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        bucket_cap_mb: 125\n",
      "        overlap_grad_sync: true\n",
      "        overlap_param_sync: true\n",
      "        contiguous_grad_buffer: true\n",
      "        contiguous_param_buffer: true\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_ratio: 0.1\n",
      "          constant_steps: 0\n",
      "          min_lr: 1.0e-05\n",
      "      data:\n",
      "        data_impl: mmap\n",
      "        splits_string: 99,1,1\n",
      "        seq_length: 4096\n",
      "        skip_warmup: true\n",
      "        num_workers: 2\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        index_mapping_dir: null\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:41 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_init:352] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1015 07:06:41.422438143 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:41 tokenizer_utils:188] Getting SentencePiece with model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "[NeMo I 2024-10-15 07:06:41 megatron_base_model:584] Padded vocab_size: 32128, original vocab_size: 32003, dummy tokens: 125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:498] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:41 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:06:42 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:42 build_model:143]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 469256192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `MegatronGPTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:06:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:163: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:06:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints exists and is not empty.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1592] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 4.69e+08. Number of precise model parameters on device: 469256192.\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1446] Building GPT datasets.\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] Let split_matrix = [(0, 0.9801980198019802), (0.9801980198019802, 0.9900990099009901), (0.9900990099009901, 1.0)]\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] Building dataset splits with cls=GPTDataset, sizes=[100, 64, 50], and config=GPTDatasetConfig(random_seed=1234, sequence_length=4096, blend=(['/global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document'], [1.0]), blend_per_split=None, split='99,1,1', split_matrix=[(0, 0.9801980198019802), (0.9801980198019802, 0.9900990099009901), (0.9900990099009901, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.sentencepiece_tokenizer.SentencePieceTokenizer object at 0x518993e76560>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True)\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] Load the _IndexReader from /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document.idx\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tExtract the sequence lengths\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tExtract the sequence pointers\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tExtract the document indices\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] > total number of sequences: 1000\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] > total number of documents: 1000\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] Load the GPTDataset train indices\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the document index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-document_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the sample index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the shuffle index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] > total number of samples: 803\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] Load the GPTDataset valid indices\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the document index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-document_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the sample index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the shuffle index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] > total number of samples: 72\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] Load the GPTDataset test indices\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the document index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-document_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the sample index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tLoad the shuffle index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] > total number of samples: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:43 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:43 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:43 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-10-15 07:06:43 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:43 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:43 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-10-15 07:06:43 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:43 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:06:43 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:43 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1530] Length of train dataset: 101\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1532] Length of val dataset: 65\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1534] Length of test dataset: 51\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1535] Finished building GPT datasets.\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1636] Setting up train dataloader with len(len(self._train_ds)): 101 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:06:43 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 101 and consumed_samples: 0\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1644] Setting up validation dataloader with len(len(self._validation_ds)): 65 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:06:43 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 65 and consumed_samples: 0\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1665] Setting up test dataloader with len(len(self._test_ds)): 51 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:06:43 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:06:43 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 51 and consumed_samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:06:43 modelPT:770] Optimizer config = MegatronDistributedFusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 0.0003747454131290244\n",
      "        weight_decay: 0.00018931221346043012\n",
      "    )\n",
      "[NeMo I 2024-10-15 07:06:43 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x5189ffbff460>\" \n",
      "    will be used during training (effective maximum steps = 100) - \n",
      "    Parameters : \n",
      "    (warmup_ratio: 0.1793158753109551\n",
      "    constant_steps: 0\n",
      "    min_lr: 1.0e-05\n",
      "    max_steps: 100\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | Float16Module | 469 M \n",
      "----------------------------------------\n",
      "469 M     Trainable params\n",
      "0         Non-trainable params\n",
      "469 M     Total params\n",
      "1,877.025 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0033617019653320312,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:06:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:06:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0035228729248046875,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28afee0b79df4579b0e636596b1f69e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.003549814224243164,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'val_loss' reached 5.99272 (best 5.99272), saving model to '/global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints/megatron_llama--val_loss=5.99-step=100-consumed_samples=100.0.ckpt' as top 10\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "[I 2024-10-15 07:07:14,463] Trial 1 finished with value: 5.992722988128662 and parameters: {'learning_rate': 0.0003747454131290244, 'weight_decay': 0.00018931221346043012, 'warmup_ratio': 0.1793158753109551}. Best is trial 0 with value: 5.908960819244385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run:\n",
      "  name: llama2_7b\n",
      "  results_dir: /global/scratch/users/ksevegnani/nemo_test/out\n",
      "  time_limit: 0-01:30:00\n",
      "  dependency: singleton\n",
      "trainer:\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  accelerator: gpu\n",
      "  precision: bf16\n",
      "  logger: false\n",
      "  enable_checkpointing: false\n",
      "  use_distributed_sampler: false\n",
      "  max_epochs: null\n",
      "  max_steps: 100\n",
      "  max_time: 05:23:30:00\n",
      "  log_every_n_steps: 10\n",
      "  val_check_interval: 100\n",
      "  limit_val_batches: 32\n",
      "  limit_test_batches: 50\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 1.0\n",
      "exp_manager:\n",
      "  explicit_log_dir: ${run.results_dir}/results\n",
      "  exp_dir: null\n",
      "  name: megatron_llama\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    project: nemo_llama_pretrain\n",
      "    name: ${run.name}\n",
      "  resume_if_exists: false\n",
      "  resume_ignore_no_checkpoint: true\n",
      "  create_checkpoint_callback: true\n",
      "  checkpoint_callback_params:\n",
      "    monitor: val_loss\n",
      "    save_top_k: 10\n",
      "    mode: min\n",
      "    always_save_nemo: false\n",
      "    save_nemo_on_train_end: false\n",
      "    filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "  log_step_timing: true\n",
      "  step_timing_kwargs:\n",
      "    sync_cuda: true\n",
      "    buffer_size: 5\n",
      "model:\n",
      "  mcore_gpt: true\n",
      "  micro_batch_size: 1\n",
      "  global_batch_size: 1\n",
      "  rampup_batch_size: null\n",
      "  tensor_model_parallel_size: 1\n",
      "  pipeline_model_parallel_size: 1\n",
      "  virtual_pipeline_model_parallel_size: null\n",
      "  encoder_seq_length: 2048\n",
      "  max_position_embeddings: 2048\n",
      "  num_layers: 4\n",
      "  hidden_size: 2048\n",
      "  ffn_hidden_size: 11008\n",
      "  num_attention_heads: 32\n",
      "  init_method_std: 0.01\n",
      "  use_scaled_init_method: true\n",
      "  hidden_dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  ffn_dropout: 0.0\n",
      "  kv_channels: null\n",
      "  apply_query_key_layer_scaling: true\n",
      "  normalization: rmsnorm\n",
      "  layernorm_epsilon: 1.0e-05\n",
      "  do_layer_norm_weight_decay: false\n",
      "  make_vocab_size_divisible_by: 128\n",
      "  pre_process: true\n",
      "  post_process: true\n",
      "  persist_layer_norm: true\n",
      "  bias: false\n",
      "  activation: fast-swiglu\n",
      "  headscale: false\n",
      "  transformer_block_type: pre_ln\n",
      "  openai_gelu: false\n",
      "  normalize_attention_scores: true\n",
      "  position_embedding_type: rope\n",
      "  rotary_percentage: 1.0\n",
      "  apply_rope_fusion: true\n",
      "  attention_type: multihead\n",
      "  share_embeddings_and_output_weights: false\n",
      "  tokenizer:\n",
      "    library: sentencepiece\n",
      "    type: null\n",
      "    model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "    delimiter: null\n",
      "    vocab_file: null\n",
      "    merge_file: null\n",
      "    sentencepiece_legacy: false\n",
      "  native_amp_init_scale: 4294967296\n",
      "  native_amp_growth_interval: 1000\n",
      "  hysteresis: 2\n",
      "  fp32_residual_connection: false\n",
      "  fp16_lm_cross_entropy: false\n",
      "  megatron_amp_O2: true\n",
      "  grad_allreduce_chunk_size_mb: 125\n",
      "  grad_div_ar_fusion: true\n",
      "  gradient_accumulation_fusion: true\n",
      "  bias_activation_fusion: true\n",
      "  bias_dropout_add_fusion: true\n",
      "  masked_softmax_fusion: true\n",
      "  seed: 1234\n",
      "  resume_from_checkpoint: null\n",
      "  use_cpu_initialization: false\n",
      "  onnx_safe: false\n",
      "  apex_transformer_log_level: 30\n",
      "  gradient_as_bucket_view: true\n",
      "  sync_batch_comm: false\n",
      "  activations_checkpoint_granularity: null\n",
      "  activations_checkpoint_method: block\n",
      "  activations_checkpoint_num_layers: 0\n",
      "  num_micro_batches_with_partial_activation_checkpoints: null\n",
      "  activations_checkpoint_layers_per_pipeline: null\n",
      "  sequence_parallel: false\n",
      "  transformer_engine: true\n",
      "  fp8: true\n",
      "  fp8_e4m3: true\n",
      "  fp8_hybrid: false\n",
      "  fp8_margin: 0\n",
      "  fp8_interval: 1\n",
      "  fp8_amax_history_len: 1024\n",
      "  fp8_amax_compute_algo: max\n",
      "  use_emha: false\n",
      "  ub_tp_comm_overlap: false\n",
      "  tp_comm_atomic_ag: false\n",
      "  tp_comm_atomic_rs: false\n",
      "  use_flash_attention: true\n",
      "  optim:\n",
      "    name: distributed_fused_adam\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.1\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.95\n",
      "    bucket_cap_mb: 125\n",
      "    overlap_grad_sync: true\n",
      "    overlap_param_sync: true\n",
      "    contiguous_grad_buffer: true\n",
      "    contiguous_param_buffer: true\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_ratio: 0.1\n",
      "      constant_steps: 0\n",
      "      min_lr: 1.0e-05\n",
      "  data:\n",
      "    data_impl: mmap\n",
      "    splits_string: 99,1,1\n",
      "    seq_length: 4096\n",
      "    skip_warmup: true\n",
      "    num_workers: 2\n",
      "    dataloader_type: single\n",
      "    reset_position_ids: false\n",
      "    reset_attention_mask: false\n",
      "    eod_mask_loss: false\n",
      "    index_mapping_dir: null\n",
      "    data_prefix:\n",
      "    - 1.0\n",
      "    - /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document\n",
      "\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_trainer_builder:58] Detected interactive environment, using NLPDDPStrategyNotebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:14 dist_ckpt_io:320] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:14 exp_manager:341] ExpManager schema\n",
      "[NeMo I 2024-10-15 07:07:14 exp_manager:342] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:14 exp_manager:712] Exp_manager is logging to /global/scratch/users/ksevegnani/nemo_test/out/results, but it already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:14 exp_manager:400] Experiments will be logged at /global/scratch/users/ksevegnani/nemo_test/out/results\n",
      "[NeMo I 2024-10-15 07:07:14 exp_manager:860] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:14 exp_manager:970] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:14 3748006741:2] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-15 07:07:14 3748006741:3] \n",
      "    run:\n",
      "      name: llama2_7b\n",
      "      results_dir: /global/scratch/users/ksevegnani/nemo_test/out\n",
      "      time_limit: 0-01:30:00\n",
      "      dependency: singleton\n",
      "    trainer:\n",
      "      num_nodes: 1\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 100\n",
      "      max_time: 05:23:30:00\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 32\n",
      "      limit_test_batches: 50\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: ${run.results_dir}/results\n",
      "      exp_dir: null\n",
      "      name: megatron_llama\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: nemo_llama_pretrain\n",
      "        name: ${run.name}\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "      log_step_timing: true\n",
      "      step_timing_kwargs:\n",
      "        sync_cuda: true\n",
      "        buffer_size: 5\n",
      "    model:\n",
      "      mcore_gpt: true\n",
      "      micro_batch_size: 1\n",
      "      global_batch_size: 1\n",
      "      rampup_batch_size: null\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 2048\n",
      "      max_position_embeddings: 2048\n",
      "      num_layers: 4\n",
      "      hidden_size: 2048\n",
      "      ffn_hidden_size: 11008\n",
      "      num_attention_heads: 32\n",
      "      init_method_std: 0.01\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      normalization: rmsnorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      bias: false\n",
      "      activation: fast-swiglu\n",
      "      headscale: false\n",
      "      transformer_block_type: pre_ln\n",
      "      openai_gelu: false\n",
      "      normalize_attention_scores: true\n",
      "      position_embedding_type: rope\n",
      "      rotary_percentage: 1.0\n",
      "      apply_rope_fusion: true\n",
      "      attention_type: multihead\n",
      "      share_embeddings_and_output_weights: false\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "        delimiter: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        sentencepiece_legacy: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: true\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: true\n",
      "      gradient_accumulation_fusion: true\n",
      "      bias_activation_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: null\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: block\n",
      "      activations_checkpoint_num_layers: 0\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: false\n",
      "      transformer_engine: true\n",
      "      fp8: true\n",
      "      fp8_e4m3: true\n",
      "      fp8_hybrid: false\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1024\n",
      "      fp8_amax_compute_algo: max\n",
      "      use_emha: false\n",
      "      ub_tp_comm_overlap: false\n",
      "      tp_comm_atomic_ag: false\n",
      "      tp_comm_atomic_rs: false\n",
      "      use_flash_attention: true\n",
      "      optim:\n",
      "        name: distributed_fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.1\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        bucket_cap_mb: 125\n",
      "        overlap_grad_sync: true\n",
      "        overlap_param_sync: true\n",
      "        contiguous_grad_buffer: true\n",
      "        contiguous_param_buffer: true\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_ratio: 0.1\n",
      "          constant_steps: 0\n",
      "          min_lr: 1.0e-05\n",
      "      data:\n",
      "        data_impl: mmap\n",
      "        splits_string: 99,1,1\n",
      "        seq_length: 4096\n",
      "        skip_warmup: true\n",
      "        num_workers: 2\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        index_mapping_dir: null\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:14 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_init:352] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1015 07:07:14.341718621 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:14 tokenizer_utils:188] Getting SentencePiece with model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "[NeMo I 2024-10-15 07:07:14 megatron_base_model:584] Padded vocab_size: 32128, original vocab_size: 32003, dummy tokens: 125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:498] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:14 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:14 build_model:143]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 469256192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `MegatronGPTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:07:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:163: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:07:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints exists and is not empty.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1592] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 4.69e+08. Number of precise model parameters on device: 469256192.\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1446] Building GPT datasets.\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] Let split_matrix = [(0, 0.9801980198019802), (0.9801980198019802, 0.9900990099009901), (0.9900990099009901, 1.0)]\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] Building dataset splits with cls=GPTDataset, sizes=[100, 64, 50], and config=GPTDatasetConfig(random_seed=1234, sequence_length=4096, blend=(['/global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document'], [1.0]), blend_per_split=None, split='99,1,1', split_matrix=[(0, 0.9801980198019802), (0.9801980198019802, 0.9900990099009901), (0.9900990099009901, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.sentencepiece_tokenizer.SentencePieceTokenizer object at 0x51867caa1030>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True)\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] Load the _IndexReader from /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document.idx\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tExtract the sequence lengths\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tExtract the sequence pointers\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tExtract the document indices\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] > total number of sequences: 1000\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] > total number of documents: 1000\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] Load the GPTDataset train indices\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the document index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-document_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the sample index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the shuffle index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] > total number of samples: 803\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] Load the GPTDataset valid indices\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the document index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-document_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the sample index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the shuffle index from 57f89e2d2b26b4a74f1352f5a4c9c034-GPTDataset-valid-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] > total number of samples: 72\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] Load the GPTDataset test indices\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the document index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-document_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the sample index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tLoad the shuffle index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] > total number of samples: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:16 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:16 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:16 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-10-15 07:07:16 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:16 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:16 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-10-15 07:07:16 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:16 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:07:16 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:16 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1530] Length of train dataset: 101\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1532] Length of val dataset: 65\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1534] Length of test dataset: 51\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1535] Finished building GPT datasets.\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1636] Setting up train dataloader with len(len(self._train_ds)): 101 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:16 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 101 and consumed_samples: 0\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1644] Setting up validation dataloader with len(len(self._validation_ds)): 65 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:16 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 65 and consumed_samples: 0\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1665] Setting up test dataloader with len(len(self._test_ds)): 51 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:16 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:16 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 51 and consumed_samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:16 modelPT:770] Optimizer config = MegatronDistributedFusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 0.0009889393275289025\n",
      "        weight_decay: 0.0002050977932188532\n",
      "    )\n",
      "[NeMo I 2024-10-15 07:07:16 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x518993e1d540>\" \n",
      "    will be used during training (effective maximum steps = 100) - \n",
      "    Parameters : \n",
      "    (warmup_ratio: 0.1530318683315003\n",
      "    constant_steps: 0\n",
      "    min_lr: 1.0e-05\n",
      "    max_steps: 100\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | Float16Module | 469 M \n",
      "----------------------------------------\n",
      "469 M     Trainable params\n",
      "0         Non-trainable params\n",
      "469 M     Total params\n",
      "1,877.025 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0033288002014160156,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:17 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:07:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0030689239501953125,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48bd26d5ac943e0b3012329b9b70a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0037577152252197266,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'val_loss' reached 6.15920 (best 6.15920), saving model to '/global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints/megatron_llama--val_loss=6.16-step=100-consumed_samples=100.0.ckpt' as top 10\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "[I 2024-10-15 07:07:51,463] Trial 2 finished with value: 6.159204006195068 and parameters: {'learning_rate': 0.0009889393275289025, 'weight_decay': 0.0002050977932188532, 'warmup_ratio': 0.1530318683315003}. Best is trial 0 with value: 5.908960819244385.\n"
     ]
    }
   ],
   "source": [
    "# Optimize the hyperparameters\n",
    "study.optimize(objective, n_trials=3, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.0003654503479685462, 'weight_decay': 0.0009979655831599337, 'warmup_ratio': 0.1911978919333179}\n",
      "Best validation loss: 5.908960819244385\n"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameters and corresponding validation loss\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "run:\n",
      "  name: llama2_7b\n",
      "  results_dir: /global/scratch/users/ksevegnani/nemo_test/out\n",
      "  time_limit: 0-01:30:00\n",
      "  dependency: singleton\n",
      "trainer:\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  accelerator: gpu\n",
      "  precision: bf16\n",
      "  logger: false\n",
      "  enable_checkpointing: false\n",
      "  use_distributed_sampler: false\n",
      "  max_epochs: null\n",
      "  max_steps: 100\n",
      "  max_time: 05:23:30:00\n",
      "  log_every_n_steps: 10\n",
      "  val_check_interval: 100\n",
      "  limit_val_batches: 32\n",
      "  limit_test_batches: 50\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 1.0\n",
      "exp_manager:\n",
      "  explicit_log_dir: ${run.results_dir}/results\n",
      "  exp_dir: null\n",
      "  name: megatron_llama\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    project: nemo_llama_pretrain\n",
      "    name: ${run.name}\n",
      "  resume_if_exists: false\n",
      "  resume_ignore_no_checkpoint: true\n",
      "  create_checkpoint_callback: true\n",
      "  checkpoint_callback_params:\n",
      "    monitor: val_loss\n",
      "    save_top_k: 10\n",
      "    mode: min\n",
      "    always_save_nemo: false\n",
      "    save_nemo_on_train_end: false\n",
      "    filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "  log_step_timing: true\n",
      "  step_timing_kwargs:\n",
      "    sync_cuda: true\n",
      "    buffer_size: 5\n",
      "model:\n",
      "  mcore_gpt: true\n",
      "  micro_batch_size: 1\n",
      "  global_batch_size: 1\n",
      "  rampup_batch_size: null\n",
      "  tensor_model_parallel_size: 1\n",
      "  pipeline_model_parallel_size: 1\n",
      "  virtual_pipeline_model_parallel_size: null\n",
      "  encoder_seq_length: 2048\n",
      "  max_position_embeddings: 2048\n",
      "  num_layers: 4\n",
      "  hidden_size: 2048\n",
      "  ffn_hidden_size: 11008\n",
      "  num_attention_heads: 32\n",
      "  init_method_std: 0.01\n",
      "  use_scaled_init_method: true\n",
      "  hidden_dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  ffn_dropout: 0.0\n",
      "  kv_channels: null\n",
      "  apply_query_key_layer_scaling: true\n",
      "  normalization: rmsnorm\n",
      "  layernorm_epsilon: 1.0e-05\n",
      "  do_layer_norm_weight_decay: false\n",
      "  make_vocab_size_divisible_by: 128\n",
      "  pre_process: true\n",
      "  post_process: true\n",
      "  persist_layer_norm: true\n",
      "  bias: false\n",
      "  activation: fast-swiglu\n",
      "  headscale: false\n",
      "  transformer_block_type: pre_ln\n",
      "  openai_gelu: false\n",
      "  normalize_attention_scores: true\n",
      "  position_embedding_type: rope\n",
      "  rotary_percentage: 1.0\n",
      "  apply_rope_fusion: true\n",
      "  attention_type: multihead\n",
      "  share_embeddings_and_output_weights: false\n",
      "  tokenizer:\n",
      "    library: sentencepiece\n",
      "    type: null\n",
      "    model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "    delimiter: null\n",
      "    vocab_file: null\n",
      "    merge_file: null\n",
      "    sentencepiece_legacy: false\n",
      "  native_amp_init_scale: 4294967296\n",
      "  native_amp_growth_interval: 1000\n",
      "  hysteresis: 2\n",
      "  fp32_residual_connection: false\n",
      "  fp16_lm_cross_entropy: false\n",
      "  megatron_amp_O2: true\n",
      "  grad_allreduce_chunk_size_mb: 125\n",
      "  grad_div_ar_fusion: true\n",
      "  gradient_accumulation_fusion: true\n",
      "  bias_activation_fusion: true\n",
      "  bias_dropout_add_fusion: true\n",
      "  masked_softmax_fusion: true\n",
      "  seed: 1234\n",
      "  resume_from_checkpoint: null\n",
      "  use_cpu_initialization: false\n",
      "  onnx_safe: false\n",
      "  apex_transformer_log_level: 30\n",
      "  gradient_as_bucket_view: true\n",
      "  sync_batch_comm: false\n",
      "  activations_checkpoint_granularity: null\n",
      "  activations_checkpoint_method: block\n",
      "  activations_checkpoint_num_layers: 0\n",
      "  num_micro_batches_with_partial_activation_checkpoints: null\n",
      "  activations_checkpoint_layers_per_pipeline: null\n",
      "  sequence_parallel: false\n",
      "  transformer_engine: true\n",
      "  fp8: true\n",
      "  fp8_e4m3: true\n",
      "  fp8_hybrid: false\n",
      "  fp8_margin: 0\n",
      "  fp8_interval: 1\n",
      "  fp8_amax_history_len: 1024\n",
      "  fp8_amax_compute_algo: max\n",
      "  use_emha: false\n",
      "  ub_tp_comm_overlap: false\n",
      "  tp_comm_atomic_ag: false\n",
      "  tp_comm_atomic_rs: false\n",
      "  use_flash_attention: true\n",
      "  optim:\n",
      "    name: distributed_fused_adam\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.1\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.95\n",
      "    bucket_cap_mb: 125\n",
      "    overlap_grad_sync: true\n",
      "    overlap_param_sync: true\n",
      "    contiguous_grad_buffer: true\n",
      "    contiguous_param_buffer: true\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_ratio: 0.1\n",
      "      constant_steps: 0\n",
      "      min_lr: 1.0e-05\n",
      "  data:\n",
      "    data_impl: mmap\n",
      "    splits_string: 99,1,1\n",
      "    seq_length: 4096\n",
      "    skip_warmup: true\n",
      "    num_workers: 2\n",
      "    dataloader_type: single\n",
      "    reset_position_ids: false\n",
      "    reset_attention_mask: false\n",
      "    eod_mask_loss: false\n",
      "    index_mapping_dir: null\n",
      "    data_prefix:\n",
      "    - 1.0\n",
      "    - /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document\n",
      "\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_trainer_builder:58] Detected interactive environment, using NLPDDPStrategyNotebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:51 dist_ckpt_io:320] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "cfg = get_config()\n",
    "\n",
    "# setting custom values\n",
    "cfg.trainer.max_epochs = 10\n",
    "cfg.trainer.devices = 1\n",
    "cfg.trainer.precision = 16\n",
    "cfg.trainer.accelerator = \"gpu\"\n",
    "cfg.trainer.log_every_n_steps = 10\n",
    "cfg.trainer.val_check_interval = 0.5\n",
    "\n",
    "\n",
    "best_trainer = MegatronTrainerBuilder(cfg).create_trainer()\n",
    "\n",
    "# best_trainer = pl.Trainer(\n",
    "#     max_epochs=10,\n",
    "#     gpus=1,\n",
    "#     precision=16,\n",
    "#     amp_level='O2',\n",
    "#     accelerator=\"gpu\",\n",
    "#     strategy=\"ddp\",\n",
    "#     log_every_n_steps=10,\n",
    "#     val_check_interval=0.5,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:51 3748006741:2] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-15 07:07:51 3748006741:3] \n",
      "    run:\n",
      "      name: llama2_7b\n",
      "      results_dir: /global/scratch/users/ksevegnani/nemo_test/out\n",
      "      time_limit: 0-01:30:00\n",
      "      dependency: singleton\n",
      "    trainer:\n",
      "      num_nodes: 1\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 10\n",
      "      max_steps: 100\n",
      "      max_time: 05:23:30:00\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.5\n",
      "      limit_val_batches: 32\n",
      "      limit_test_batches: 50\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: ${run.results_dir}/results\n",
      "      exp_dir: null\n",
      "      name: megatron_llama\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: nemo_llama_pretrain\n",
      "        name: ${run.name}\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "      log_step_timing: true\n",
      "      step_timing_kwargs:\n",
      "        sync_cuda: true\n",
      "        buffer_size: 5\n",
      "    model:\n",
      "      mcore_gpt: true\n",
      "      micro_batch_size: 1\n",
      "      global_batch_size: 1\n",
      "      rampup_batch_size: null\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 2048\n",
      "      max_position_embeddings: 2048\n",
      "      num_layers: 4\n",
      "      hidden_size: 2048\n",
      "      ffn_hidden_size: 11008\n",
      "      num_attention_heads: 32\n",
      "      init_method_std: 0.01\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      normalization: rmsnorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      bias: false\n",
      "      activation: fast-swiglu\n",
      "      headscale: false\n",
      "      transformer_block_type: pre_ln\n",
      "      openai_gelu: false\n",
      "      normalize_attention_scores: true\n",
      "      position_embedding_type: rope\n",
      "      rotary_percentage: 1.0\n",
      "      apply_rope_fusion: true\n",
      "      attention_type: multihead\n",
      "      share_embeddings_and_output_weights: false\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "        delimiter: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        sentencepiece_legacy: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: true\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: true\n",
      "      gradient_accumulation_fusion: true\n",
      "      bias_activation_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: null\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: block\n",
      "      activations_checkpoint_num_layers: 0\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: false\n",
      "      transformer_engine: true\n",
      "      fp8: true\n",
      "      fp8_e4m3: true\n",
      "      fp8_hybrid: false\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1024\n",
      "      fp8_amax_compute_algo: max\n",
      "      use_emha: false\n",
      "      ub_tp_comm_overlap: false\n",
      "      tp_comm_atomic_ag: false\n",
      "      tp_comm_atomic_rs: false\n",
      "      use_flash_attention: true\n",
      "      optim:\n",
      "        name: distributed_fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.1\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        bucket_cap_mb: 125\n",
      "        overlap_grad_sync: true\n",
      "        overlap_param_sync: true\n",
      "        contiguous_grad_buffer: true\n",
      "        contiguous_param_buffer: true\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_ratio: 0.1\n",
      "          constant_steps: 0\n",
      "          min_lr: 1.0e-05\n",
      "      data:\n",
      "        data_impl: mmap\n",
      "        splits_string: 99,1,1\n",
      "        seq_length: 4096\n",
      "        skip_warmup: true\n",
      "        num_workers: 2\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        index_mapping_dir: null\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:51 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_init:352] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1015 07:07:51.325239649 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:51 tokenizer_utils:188] Getting SentencePiece with model: /global/scratch/users/ksevegnani/nemo_test/llama-tokenizer.model\n",
      "[NeMo I 2024-10-15 07:07:51 megatron_base_model:584] Padded vocab_size: 32128, original vocab_size: 32003, dummy tokens: 125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:1154] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-15 07:07:51 megatron_base_model:556] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:51 build_model:143]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 469256192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tune the model with the best hyperparameters\n",
    "best_model = initialize_model(cfg, best_trainer)\n",
    "best_model.cfg.optim.lr = study.best_params[\"learning_rate\"]\n",
    "best_model.cfg.optim.weight_decay = study.best_params[\"weight_decay\"]\n",
    "best_model.cfg.optim.sched.warmup_ratio = study.best_params[\"warmup_ratio\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:51 exp_manager:341] ExpManager schema\n",
      "[NeMo I 2024-10-15 07:07:51 exp_manager:342] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-10-15 07:07:51 exp_manager:707] exp_manager received explicit_log_dir: /global/scratch/users/ksevegnani/nemo_test/out/results and at least one of exp_dir: best_model_experiment, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-10-15 07:07:51 exp_manager:712] Exp_manager is logging to /global/scratch/users/ksevegnani/nemo_test/out/results, but it already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:51 exp_manager:400] Experiments will be logged at /global/scratch/users/ksevegnani/nemo_test/out/results\n",
      "[NeMo I 2024-10-15 07:07:51 exp_manager:860] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:51 exp_manager:970] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/global/scratch/users/ksevegnani/nemo_test/out/results')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.exp_manager.exp_dir=\"best_model_experiment\"\n",
    "cfg.exp_manager.create_wandb_logger=False\n",
    "\n",
    "best_trainer.logger=None\n",
    "cfg.trainer.max_steps=\"null\"\n",
    "cfg.trainer.max_epochs=1\n",
    "exp_manager(\n",
    "    best_trainer,\n",
    "    cfg.exp_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `MegatronGPTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:07:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:163: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:07:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints exists and is not empty.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1592] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 4.69e+08. Number of precise model parameters on device: 469256192.\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1446] Building GPT datasets.\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] Let split_matrix = [(0, 0.9801980198019802), (0.9801980198019802, 0.9900990099009901), (0.9900990099009901, 1.0)]\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] Building dataset splits with cls=GPTDataset, sizes=[100, 6432.0, 50], and config=GPTDatasetConfig(random_seed=1234, sequence_length=4096, blend=(['/global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document'], [1.0]), blend_per_split=None, split='99,1,1', split_matrix=[(0, 0.9801980198019802), (0.9801980198019802, 0.9900990099009901), (0.9900990099009901, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.sentencepiece_tokenizer.SentencePieceTokenizer object at 0x51869fbdc460>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True)\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] Load the _IndexReader from /global/scratch/users/ksevegnani/nemo_test/pubmedqa_big_llama_input_document.idx\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tExtract the sequence lengths\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tExtract the sequence pointers\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tExtract the document indices\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] > total number of sequences: 1000\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] > total number of documents: 1000\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] Load the GPTDataset train indices\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tLoad the document index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-document_index.npy\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tLoad the sample index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tLoad the shuffle index from c50711825eb4ed5bf463d659292929cb-GPTDataset-train-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] > total number of samples: 803\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] Build and save the GPTDataset valid indices\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] > total number of samples: 6467\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] > total number of epochs: 808\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] Load the GPTDataset test indices\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tLoad the document index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-document_index.npy\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tLoad the sample index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-sample_index.npy\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tLoad the shuffle index from 5b905b660e0ca56c9a3811a319a4a0c1-GPTDataset-test-shuffle_index.npy\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] > total number of samples: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:53 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:53 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:53 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-10-15 07:07:53 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:53 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:53 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-10-15 07:07:53 utils:47] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:53 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-10-15 07:07:53 utils:47] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:53 utils:47] Unable to save the BlendedDataset indexes because path_to_cache is None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1530] Length of train dataset: 101\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1532] Length of val dataset: 6465\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1534] Length of test dataset: 51\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1535] Finished building GPT datasets.\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1636] Setting up train dataloader with len(len(self._train_ds)): 101 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:53 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 101 and consumed_samples: 0\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1644] Setting up validation dataloader with len(len(self._validation_ds)): 6465 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:53 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 6465 and consumed_samples: 0\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1665] Setting up test dataloader with len(len(self._test_ds)): 51 and consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:53 megatron_gpt_model:1544] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-15 07:07:53 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 51 and consumed_samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2024-10-15 07:07:53 megatron_base_model:1195] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:07:53 modelPT:770] Optimizer config = MegatronDistributedFusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 0.0003654503479685462\n",
      "        weight_decay: 0.0009979655831599337\n",
      "    )\n",
      "[NeMo I 2024-10-15 07:07:53 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x5189ffc3a5c0>\" \n",
      "    will be used during training (effective maximum steps = 100) - \n",
      "    Parameters : \n",
      "    (warmup_ratio: 0.1911978919333179\n",
      "    constant_steps: 0\n",
      "    min_lr: 1.0e-05\n",
      "    max_steps: 100\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | Float16Module | 469 M \n",
      "----------------------------------------\n",
      "469 M     Trainable params\n",
      "0         Non-trainable params\n",
      "469 M     Total params\n",
      "1,877.025 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0027801990509033203,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:07:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-10-15 07:08:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0036590099334716797,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227bf4c810f640828430285cdfc21bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-15 07:08:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-10-15 07:08:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0034868717193603516,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'val_loss' reached 7.68887 (best 7.68887), saving model to '/global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints/megatron_llama--val_loss=7.69-step=50-consumed_samples=50.0.ckpt' as top 10\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0035026073455810547,
       "initial": 0,
       "n": 0,
       "ncols": 244,
       "nrows": 96,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'val_loss' reached 7.39861 (best 7.39861), saving model to '/global/scratch/users/ksevegnani/nemo_test/out/results/checkpoints/megatron_llama--val_loss=7.40-step=100-consumed_samples=100.0.ckpt' as top 10\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n"
     ]
    }
   ],
   "source": [
    "best_trainer.fit(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-15 07:08:42 dist_ckpt_io:320] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the fine-tuned model\n",
    "best_model.save_to(\"llama2-7b-finetuned-optuna.nemo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
